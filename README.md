# expectile-regression-RL4CO


This project explores the application efficacy of the expectile regression function of the value function in reinforcement learning in combinatorial optimization tasks through empirical research. Based on the RL4CO framework, we replace the value function in the  PPO algorithm with the expectile regression function with adjustable parameters (τ), and construct  experiments on the traveling salesman problem (TSP) and the flexible job-shop scheduling problem (FJSP). 

The experimental results show that in the TSP scenario, the high-bit parameter τ=0.9 is better in the long term, which can effectively improve the path exploration ability of medium to large-scale problems, such as TSP20, TSP100, FJSP 10 jobs. In small-scale TSP tasks, such as TSP5, the conservative strategy with τ=0.5 is more conducive to stable convergence. Specifically, the FJSP problem shows non-monotony correlation. τ=0.7 to achieve the optimal scheduling scheme in small-scale complex instances, such as FJSP 3 jobs, but τ=0.5 unexpectedly outperforms the τ=0.7 scheme by 9.3% in performance gains in complex FJSP 10 jobs. However, time was limited and the experiments were insufficient, including the number of environments and the lack of sufficient repetition of the experiments. These findings reveal that the selection of tau parameters has significant problem scale sensitivity and structural dependence. 
